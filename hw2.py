# -*- coding: utf-8 -*-
"""HW2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XeXgrGt4FMfQhT_zQZRRGpxBvgKg-MDM
"""

# ============================================
# HW2 -- MNIST 生成模型大亂鬥
# 模型: VAE, GAN, cGAN, Diffusion (DDPM簡化版)
# ============================================

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from torchvision.utils import make_grid, save_image
import matplotlib.pyplot as plt
import numpy as np
import os

# 固定隨機種子
seed = 42
torch.manual_seed(seed)
np.random.seed(seed)

# 設定 device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ============================================
# 1. 資料準備
# ============================================
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset = datasets.MNIST(root="./data", train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)

# ============================================
# 2. VAE
# ============================================
class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()
        self.fc1 = nn.Linear(28*28, 400)
        self.fc_mu = nn.Linear(400, 20)
        self.fc_logvar = nn.Linear(400, 20)
        self.fc2 = nn.Linear(20, 400)
        self.fc3 = nn.Linear(400, 28*28)

    def encode(self, x):
        h1 = torch.relu(self.fc1(x))
        return self.fc_mu(h1), self.fc_logvar(h1)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps*std

    def decode(self, z):
        h2 = torch.relu(self.fc2(z))
        return torch.sigmoid(self.fc3(h2))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 28*28))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def vae_loss(recon_x, x, mu, logvar):
    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 28*28), reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + KLD

vae = VAE().to(device)
optimizer_vae = optim.Adam(vae.parameters(), lr=1e-3)

# 訓練 VAE
for epoch in range(30):  # 建議可調到30+
    vae.train()
    train_loss = 0
    for batch, _ in train_loader:
        batch = batch.to(device)
        batch = (batch + 1) / 2  # 把範圍從 [-1,1] 拉回 [0,1]
        optimizer_vae.zero_grad()
        recon, mu, logvar = vae(batch)
        loss = vae_loss(recon, batch, mu, logvar)
        loss.backward()
        train_loss += loss.item()
        optimizer_vae.step()
    print(f"Epoch {epoch+1}, Loss: {train_loss/len(train_loader.dataset):.4f}")

# 生成影像
with torch.no_grad():
    z = torch.randn(10, 20).to(device)
    sample = vae.decode(z).cpu()
    save_image(sample.view(10,1,28,28), "vae.png")

# ============================================
# 3. GAN
# ============================================
class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(100, 256), nn.ReLU(),
            nn.Linear(256, 512), nn.ReLU(),
            nn.Linear(512, 28*28), nn.Tanh()
        )
    def forward(self, z):
        return self.model(z).view(-1,1,28,28)

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(28*28, 512), nn.LeakyReLU(0.2),
            nn.Linear(512, 256), nn.LeakyReLU(0.2),
            nn.Linear(256, 1), nn.Sigmoid()
        )
    def forward(self, x):
        return self.model(x.view(-1,28*28))

G, D = Generator().to(device), Discriminator().to(device)
opt_G = optim.Adam(G.parameters(), lr=2e-4)
opt_D = optim.Adam(D.parameters(), lr=2e-4)
criterion = nn.BCELoss()

for epoch in range(30):  # 建議可調到30+
    for real, _ in train_loader:
        real = real.to(device)
        bs = real.size(0)

        # 訓練 D
        z = torch.randn(bs, 100).to(device)
        fake = G(z)
        D_real = D(real).squeeze()
        D_fake = D(fake.detach()).squeeze()
        loss_D = criterion(D_real, torch.ones(bs, device=device)) + \
                 criterion(D_fake, torch.zeros(bs, device=device))
        opt_D.zero_grad()
        loss_D.backward()
        opt_D.step()

        # 訓練 G
        D_fake = D(fake).squeeze()
        loss_G = criterion(D_fake, torch.ones(bs, device=device))
        opt_G.zero_grad()
        loss_G.backward()
        opt_G.step()
    print(f"Epoch {epoch+1}, Loss D: {loss_D:.4f}, Loss G: {loss_G:.4f}")

# 生成影像
with torch.no_grad():
    z = torch.randn(10, 100).to(device)
    fake = G(z).cpu()
    save_image(fake, "gan.png", normalize=True)

# ============================================
# 4. cGAN
# ============================================
class cGenerator(nn.Module):
    def __init__(self):
        super().__init__()
        self.label_emb = nn.Embedding(10, 10)
        self.model = nn.Sequential(
            nn.Linear(100+10, 256), nn.ReLU(),
            nn.Linear(256, 512), nn.ReLU(),
            nn.Linear(512, 28*28), nn.Tanh()
        )
    def forward(self, z, labels):
        c = self.label_emb(labels)
        return self.model(torch.cat([z, c], 1)).view(-1,1,28,28)

class cDiscriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.label_emb = nn.Embedding(10, 10)
        self.model = nn.Sequential(
            nn.Linear(28*28+10, 512), nn.LeakyReLU(0.2),
            nn.Linear(512, 256), nn.LeakyReLU(0.2),
            nn.Linear(256, 1), nn.Sigmoid()
        )
    def forward(self, x, labels):
        c = self.label_emb(labels)
        return self.model(torch.cat([x.view(x.size(0), -1), c], 1))

cG, cD = cGenerator().to(device), cDiscriminator().to(device)
opt_cG = optim.Adam(cG.parameters(), lr=2e-4)
opt_cD = optim.Adam(cD.parameters(), lr=2e-4)

for epoch in range(30):  # 建議可調到30+
    for real, labels in train_loader:
        real, labels = real.to(device), labels.to(device)
        bs = real.size(0)

        # D
        z = torch.randn(bs, 100).to(device)
        fake = cG(z, labels)
        D_real = cD(real, labels).squeeze()
        D_fake = cD(fake.detach(), labels).squeeze()
        loss_D = criterion(D_real, torch.ones(bs, device=device)) + \
                 criterion(D_fake, torch.zeros(bs, device=device))
        opt_cD.zero_grad()
        loss_D.backward()
        opt_cD.step()

        # G
        D_fake = cD(fake, labels).squeeze()
        loss_G = criterion(D_fake, torch.ones(bs, device=device))
        opt_cG.zero_grad()
        loss_G.backward()
        opt_cG.step()

# 生成數字 0-9
with torch.no_grad():
    z = torch.randn(100, 100).to(device)
    labels = torch.arange(0, 10).repeat_interleave(10).to(device)
    fake = cG(z, labels).cpu()
    save_image(fake, "cgan.png", nrow=10, normalize=True)

# ============================================
# 5. Diffusion (簡化版 DDPM)
# ============================================
# 這裡寫一個簡單的 U-Net 去噪模型 (省略詳細，簡化示範)
# 建議課堂作業可使用已有的 DDPM 實作模組
# ============================================
# 5. Diffusion (簡化版 DDPM) - 補上
# ============================================
import math
import torch.nn.functional as F

# ---- hyperparams (demo-friendly) ----
T = 50                      # diffusion steps (示範用 50，品質可用 100+)
diffusion_lr = 2e-4
diffusion_epochs = 30        # demo: 5 epochs；交作業前請提高到 30+
batch_size = 128

# ---- beta schedule & precomputed terms ----
betas = torch.linspace(1e-4, 0.02, T, device=device)  # linear schedule
alphas = 1.0 - betas
alphas_cumprod = torch.cumprod(alphas, dim=0)
sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)
sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)

# helper: get scalars for a batch of t
def extract(a, t, x_shape):
    # a: tensor length T, t: (bs,) long, returns shape (bs,1,1,1)
    batch_size = t.shape[0]
    out = a[t].reshape(batch_size, 1, 1, 1)
    return out

# ---- timestep embedding (sinusoidal) ----
def timestep_embedding(timesteps, dim=128):
    # timesteps: (bs,) long
    device = timesteps.device
    half = dim // 2
    emb = math.log(10000) / (half - 1)
    emb = torch.exp(torch.arange(half, device=device) * -emb)
    emb = timesteps.float().unsqueeze(1) * emb.unsqueeze(0)  # (bs, half)
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
    if dim % 2 == 1:
        emb = F.pad(emb, (0,1))
    return emb  # (bs, dim)

# ---- 簡化 UNet-like denoiser ----
class SimpleUNet(nn.Module):
    def __init__(self, in_ch=1, base_ch=64, time_emb_dim=128):
        super().__init__()
        self.time_mlp = nn.Sequential(
            nn.Linear(time_emb_dim, time_emb_dim),
            nn.ReLU()
        )
        # time projectors to different channel sizes
        self.tp1 = nn.Linear(time_emb_dim, base_ch)
        self.tp2 = nn.Linear(time_emb_dim, base_ch*2)
        self.tp3 = nn.Linear(time_emb_dim, base_ch*4)

        # encoder
        self.conv1 = nn.Conv2d(in_ch, base_ch, 3, padding=1)
        self.conv2 = nn.Conv2d(base_ch, base_ch*2, 3, stride=2, padding=1)  # 14x14
        self.conv3 = nn.Conv2d(base_ch*2, base_ch*4, 3, stride=2, padding=1) # 7x7

        # decoder
        self.up1 = nn.Conv2d(base_ch*4, base_ch*2, 3, padding=1)
        self.up2 = nn.Conv2d(base_ch*2, base_ch, 3, padding=1)
        self.out = nn.Conv2d(base_ch, in_ch, 1)

        self.act = nn.ReLU()

    def forward(self, x, t):
        # x: (bs,1,28,28) in [-1,1], t: (bs,) long
        te = timestep_embedding(t, dim=128)
        te = self.time_mlp(te)
        # encoder
        h1 = self.conv1(x)                          # (bs, base, 28,28)
        h1 = h1 + self.tp1(te).unsqueeze(-1).unsqueeze(-1)
        h1 = self.act(h1)

        h2 = self.conv2(h1)                         # (bs, base*2,14,14)
        h2 = h2 + self.tp2(te).unsqueeze(-1).unsqueeze(-1)
        h2 = self.act(h2)

        h3 = self.conv3(h2)                         # (bs, base*4,7,7)
        h3 = h3 + self.tp3(te).unsqueeze(-1).unsqueeze(-1)
        h3 = self.act(h3)

        # decoder (upsample by interpolation)
        d1 = F.interpolate(self.up1(h3), scale_factor=2, mode='nearest')  # -> 14x14
        d1 = self.act(d1 + h2)  # simple skip
        d2 = F.interpolate(self.up2(d1), scale_factor=2, mode='nearest')  # -> 28x28
        d2 = self.act(d2 + h1)
        out = self.out(d2)
        # predict noise (same range as x)
        return out

# ---- build model & optimizer ----
denoiser = SimpleUNet().to(device)
opt_diff = optim.Adam(denoiser.parameters(), lr=diffusion_lr)
mse = nn.MSELoss()

# ---- q_sample: produce x_t from x0 and noise ----
def q_sample(x0, t, noise):
    # x0: (bs,1,28,28)
    sqrt_acp = extract(sqrt_alphas_cumprod, t, x0.shape)
    sqrt_omac = extract(sqrt_one_minus_alphas_cumprod, t, x0.shape)
    return sqrt_acp * x0 + sqrt_omac * noise

# ---- training loop (predict noise) ----
print("Start Diffusion training (simplified DDPM)...")
for epoch in range(diffusion_epochs):
    denoiser.train()
    total_loss = 0.0
    for batch, _ in train_loader:
        batch = batch.to(device)  # already normalized to [-1,1] earlier
        bs = batch.size(0)
        t = torch.randint(0, T, (bs,), device=device).long()
        noise = torch.randn_like(batch)
        x_t = q_sample(batch, t, noise)

        pred_noise = denoiser(x_t, t)  # predict noise
        loss = mse(pred_noise, noise)

        opt_diff.zero_grad()
        loss.backward()
        opt_diff.step()

        total_loss += loss.item() * bs

    print(f"Epoch {epoch+1}/{diffusion_epochs}, avg loss: {total_loss/len(train_loader.dataset):.6f}")

# ---- sampling: reverse process ----
def sample_ddpm(model, n_samples=10):
    model.eval()
    with torch.no_grad():
        x = torch.randn(n_samples, 1, 28, 28, device=device)  # start from pure noise
        for i in reversed(range(T)):
            t = torch.full((n_samples,), i, device=device, dtype=torch.long)
            # predict noise
            pred_noise = model(x, t)
            # constants (scalars for step i)
            alpha_t = alphas[i]
            alpha_cumprod_t = alphas_cumprod[i]
            sqrt_recip_alpha = 1.0 / math.sqrt(alpha_t.item())
            coef = (1 - alpha_t) / math.sqrt(1 - alpha_cumprod_t.item())

            # compute mean
            mean = sqrt_recip_alpha * (x - coef * pred_noise)

            if i > 0:
                noise = torch.randn_like(x)
                sigma = torch.sqrt(betas[i])
                x = mean + sigma * noise
            else:
                x = mean
        # convert from [-1,1] to [0,1]
        x = (x + 1.0) / 2.0
        x = x.clamp(0.0, 1.0)
        return x.cpu()

# 生成 10 張並存檔
samples = sample_ddpm(denoiser, n_samples=10)
save_image(samples, "diffusion.png", nrow=5)
print("Saved diffusion.png")
# ============================================
# 6. 結果對照
# ============================================
import matplotlib.image as mpimg

fig, axes = plt.subplots(1,4, figsize=(15,5))
for i, name in enumerate(["vae.png","gan.png","cgan.png","diffusion.png"]):
    img = mpimg.imread(name)
    axes[i].imshow(img)
    axes[i].axis("off")
    axes[i].set_title(name.split(".")[0])
plt.show()